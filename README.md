# dog-breed-classifier

## Projet de Classification des Races de Chiens

Ce repository contient un projet de classification de races de chiens √† l'aide de mod√®les de Deep Learning. Le projet est structur√© comme suit :

---

## Structure du Repository

### üìÅ notebooks/
Contient les diff√©rents notebooks Jupyter utilis√©s pour le d√©veloppement et l'exp√©rimentation.
- `train_dog_classifier.ipynb` : Notebook final regroupant le pr√©traitement des donn√©es, l'entra√Ænement et l'√©valuation des mod√®les, ainsi que le monitoring via MLFlow.

### üìÅ src/
Contient les fonctions et modules utilis√©s pour le traitement des donn√©es et la construction des mod√®les :
- `make_model_densenet.py` : Cr√©ation et entra√Ænement du mod√®le **DenseNet**.
- `make_model_mobilenet.py` : Cr√©ation et entra√Ænement du mod√®le **MobileNetV2**.
- `make_model_vgg16.py` : Cr√©ation et entra√Ænement du mod√®le **VGG16**.
- `make_dataset.py` : Chargement et s√©paration du dataset (train/val/test).
- `eval_metrics.py` : Fonction d‚Äô√©valuation des performances des mod√®les.

### üìÅ settings/
Contient les param√®tres de configuration pour l'entra√Ænement :
- `config.py` : Fichier de configuration des hyperparam√®tres et chemins.

### üìÅ api/
Repr√©sente le **backend FastAPI**. Il expose une API REST permettant de faire des pr√©dictions √† partir d'une image de chien.
- `main.py` : Fichier principal de l'API FastAPI avec l‚Äôendpoint `/predict`.
- `model_loader.py` : Chargement du mod√®le entra√Æn√©.
- `utils.py` : Fonctions utilitaires pour la pr√©diction (pr√©traitement d‚Äôimage, etc.).

### üìÅ frontend/
Repr√©sente l‚Äô**interface utilisateur** d√©velopp√©e avec React (ou tout autre framework frontend).
- Permet √† l‚Äôutilisateur d‚Äôuploader une image et de voir la race pr√©dite par le mod√®le via l‚ÄôAPI.

### üìÑ run_dog_classification.sh
Script shell pour ex√©cuter les diff√©rentes √©tapes de la pipeline ML.

### üìÑ docker-compose.yml
Fichier de composition Docker permettant de d√©marrer facilement tous les services du projet :
- **backend (API)** : Serveur FastAPI exposant l‚Äôendpoint `/predict`.
- **frontend** : Interface web utilisateur.
- (optionnel) **MLflow** ou **PostgreSQL** si tu veux √©tendre la stack.

Ce fichier permet de d√©marrer l‚Äôensemble du projet avec une seule commande : ```bash docker-compose up --build


Le mod√®le entra√Æn√© est disponible sur Google Drive. Vous pouvez le t√©l√©charger en utilisant le lien ci-dessous :

[T√©l√©charger le mod√®le entra√Æn√© (model_best.keras)](https://drive.google.com/file/d/1IkLReT9dNKQiimy8Vl5Oi9MAz0BNAr2v/view?usp=sharing)

### Instructions pour Utiliser le Mod√®le

1. T√©l√©chargez le mod√®le √† partir du lien ci-dessus.
2. Placez le fichier `model_best.keras` dans le r√©pertoire `streamlit_app`.

## Obtenir le Dataset

Le projet utilise le dataset Garbage Classification de Kaggle pour l'entra√Ænement et la validation du mod√®le. Vous pouvez le t√©l√©charger √† partir du lien suivant :

[**T√©l√©charger le Dataset**](https://www.kaggle.com/datasets/asdasdasasdas/garbage-classification?datasetId=81794&sortBy=voteCount)

### Structure du Dataset

Le dataset contient des images classifi√©es en 6 cat√©gories. Voici la r√©partition des classes :

- **Cardboard**: 403 images
- **Glass**: 501 images
- **Metal**: 410 images
- **Paper**: 594 images
- **Plastic**: 482 images
- **Trash**: 137 images

Chaque image est plac√©e dans un r√©pertoire correspondant √† sa classe. Assurez-vous que la structure des dossiers refl√®te cette distribution pour un traitement correct des donn√©es.


## Instructions d'Utilisation

1. **Installation des D√©pendances** : 
   Assurez-vous que vous avez toutes les d√©pendances n√©cessaires en installant les packages list√©s dans `requirements.txt` :
   ```bash
   pip install -r requirements.txt

2. **Execution des notebooks** : 

   - **Jupyter** :  
   Pour ex√©cuter les notebooks, vous pouvez utiliser Jupyter Notebook

   - **CLI**:  
  Pour obtenir une version des r√©sultats de l'ex√©cution du notebook, vous pouvez utiliser le fichier `run_garbage_classification.sh`. Ce script utilise la biblioth√®que `papermill` pour ex√©cuter le notebook et sauvegarder les r√©sultats dans le dossier `logs` (qui sera automatiquement cr√©√© si n√©cessaire).  

  a. Placez-vous dans le dossier `MLOPS_PROJECT`.  
  b. Ex√©cutez les commandes suivantes :


        sh run_garbage_classification.sh ou ./run_garbage_classification.sh
